# 自然语言和单词的分布式表示
## 什么是自然语言处理
> 自然语言处理（英语：Natural Language Processing，缩写作 NLP）是人工智能和语言学领域的分支学科。此领域探讨如何处理及运用自然语言；自然语言处理包括多方面和步骤，基本有认知、理解、生成等部分。
自然语言认知和理解是让电脑把输入的语言变成有意思的符号和关系，然后根据目的再处理。自然语言生成系统则是把计算机数据转化为自然语言
## NLP的组成
> NLP = NLU + NLG. NLU负责理解内容，NLG负责生成内容
### NLU
> 自然语言理解
### NLG
> NLG 是为了跨越人类和机器之间的沟通鸿沟，将非语言格式的数据转换成人类可以理解的语言格式，如文章、报告等
#### 自然语言生成的两种方式
1. text – to – text：文本到语言的生成
2. data – to – text ：数据到语言的生成
#### NLG 的3个level
1. **简单的数据合并**
> 自然语言处理的简化形式，这将允许将数据转换为文本（通过类似Excel的函数）。为了关联，以邮件合并（MS Word mailmerge）为例，其中间隙填充了一些数据，这些数据是从另一个源（例如MS Excel中的表格）中检索的
2. **模块化的NLG**
> 这种形式的NLG使用模板驱动模式来显示输出。以足球比赛得分板为例。数据动态地保持更改，并由预定义的业务规则集（如if / else循环语句）生成
3. **高级 NLG**
> 这种形式的自然语言生成就像人类一样。它理解意图，添加智能，考虑上下文，并将结果呈现在用户可以轻松阅读和理解的富有洞察力的叙述中
#### NLG的6个步骤
1. 内容确定 – Content Determination
> NLG 系统需要决定哪些信息应该包含在正在构建的文本中，哪些不应该包含。通常数据中包含的信息比最终传达的信息要多
2. 文本结构 – Text Structuring
> 确定需要传达哪些信息后，NLG 系统需要合理的组织文本的顺序。例如在报道一场篮球比赛时，会优先表达「什么时间」「什么地点」「哪2支球队」，然后再表达「比赛的概况」，最后表达「比赛的结局」
3.句子聚合 – Sentence Aggregation
> 不是每一条信息都需要一个独立的句子来表达，将多个信息合并到一个句子里表达可能会更加流畅，也更易于阅读
4. 语法化 – Lexicalisation
> 当每一句的内容确定下来后，就可以将这些信息组织成自然语言了。这个步骤会在各种信息之间加一些连接词，看起来更像是一个完整的句子
5. 参考表达式生成 – Referring Expression Generation|REG
> 这个步骤跟语法化很相似，都是选择一些单词和短语来构成一个完整的句子。不过他跟语法化的本质区别在于“REG需要识别出内容的领域，然后使用该领域（而不是其他领域）的词汇”
6. 语言实现 – Linguistic Realisation
> 当所有相关的单词和短语都已经确定时，需要将它们组合起来形成一个结构良好的完整句子
## 应用场景
* 机器翻译
* 搜索引擎
* 语音助手
## 自然语言处理技术与方法
### 语法技术
* 解析                
* 分词
  我/要/去/北京
* 断句
* 形态分割
* 词干
### 语义技术
* 词义消歧
* 命名实体识别
* 自然语言生成
## 单词含义表示的方法
> 我们的语言是由文字构成的，而语言的含义是由单词构成的。换句话说，单词是含义的最小单位。因此，为了让计算机理解自然语言，让它理解单词含义可以说是最重要的事情了
### 基于同义词词典的方法
> 要表示单词含义，首先可以考虑通过人工方式来定义单词含义。一种方法是像《新华字典》那样，一个词一个词地说明单词含义。比如，当你用字典查“汽车”这个单词时，就会看到“装有车轮并依靠它们前行的交通工具或运输工具……”这样的说明。通过像这样定义单词，计算机或许也能够理解单词含义
像这样，通过对所有单词创建近义词集合，并用图表示各个单词的关系，可以定义单词之间的联系。利用这个“单词网络”，可以教会计算机单词之间的相关性
#### WordNet
在自然语言处理领域，最著名的同义词词典是WordNet.  使用WordNet，可以获得单词的近义词，或者利用单词网络。使用单词网络，可以计算单词之间的相似度
#### 同义词词典的问题
* 语言的含义也会随着时间的推移而变化
* 词义需要标注,人力成本高
* 无法表示单词的微妙差异
### 基于计数的方法
#### 语料库
> 语料库就是大量的文本数据。不过，语料库并不是胡乱收集数据，一般收集的都是用于自然语言处理研究和应用的文本数据。比如:  中文人名语料库
语料库中包含了大量的关于自然语言的实践知识，即文章的写作方法、单词的选择方法和单词含义等。基于计数的方法的目标就是从这些富有实践知识的语料库中，自动且高效地提取本质
#### 单词的分布式表示
> 考虑如何基于分布式假设使用向量表示单词，最直截了当的实现方法是对周围单词的数量进行计数。具体来说，在关注某个单词的情况下，对它的周围出现了多少次什么单词进行计数，然后再汇总。这里，我们将这种做法称为“基于计数的方法”，或者称为“基于统计的方法”
构建一个密集向量（Dense Vector）来表征一个词汇,密集向量指向量的各个元素（大多数）是由非0实数表示的 例如: you say goodbye and I say hello.

* 可以用0100000表示you
#### 向量间的相似度
##### 余弦相似度
```python
import spacy

nlp = spacy.load("zh_core_web_lg")
doc1 = nlp("最近有什么新的电影吗？")
doc2 = nlp("最近新上映什么电影")
doc3 = nlp("我去看电影了")

print(doc1.similarity(doc2))
print(doc1.similarity(doc3))
print(doc2.similarity(doc3))
```

### 基于推理的方法

**you  ?  goodbye and I say hello**

> 当给出周围的单词（上下文）时，预测“?”处会出现什么单词，这就是推理.基于推理的方法引入了某种模型，我们将神经网络用于此模型。这个模型接收上下文信息作为输入，并输出（可能出现的）各个单词的出现概率。在这样的框架中，使用语料库来学习模型，使之能做出正确的预测
### 代码示例
```python
## 分词
from spacy.lang.zh import Chinese

# pip install https://github.com/lancopku/pkuseg-python/archive/master.zip

# Load pkuseg model in spaCy Chinese tokenizer
cfg = {"segmenter": "pkuseg"}
nlp = Chinese.from_config({"nlp": {"tokenizer": cfg}})
nlp.tokenizer.initialize(pkuseg_model="mixed")

# seg = pkuseg.pkuseg()
# text = seg.cut('我爱北京天安门')
# print(text)

doc = nlp("兵乓球拍卖完了")
for token in doc:
    print(token)
```   
```python
## 词性
import spacy

nlp = spacy.load("zh_core_web_lg")
doc = nlp("我要去北京")

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
          token.shape_, token.is_alpha, token.is_stop)
```
```python
## 分句
import spacy

nlp = spacy.load("zh_core_web_lg")
doc = nlp("生命，也许是宇宙之间唯一就应受到崇拜的因素。生命的孕育、诞生和显示本质是一种无比激动人心的过程。生命像音乐和画面一样暗自挟带着一种命定的声调或血色，当它遇到大潮的袭卷，当它听到号角的催促时，它会顿时抖擞，露出本质的绚烂和激昂。当然，这本质更可能是卑污、懦弱、乏味的；它的主人并无选取的可能。")
assert doc.has_annotation("SENT_START")
for sent in doc.sents:
    print(sent.text)
```
```python
## 词向量
import spacy

nlp = spacy.load("zh_core_web_lg")
tokens = nlp("我要去北京")

for token in tokens:
    print(token.text, token.has_vector, token.vector_norm, token.is_oov)
```
```python
## 短语提取
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Autonomous cars shift insurance liability toward manufacturers")
for chunk in doc.noun_chunks:
    print(chunk.text, chunk.root.text, chunk.root.dep_,chunk.root.head.text)
```
```python
## 命名实体识别
import spacy

nlp = spacy.load("zh_core_web_lg")
doc = nlp("2022年我要去北京")

for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)
```

