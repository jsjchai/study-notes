## 循环神经网络 RNN

### 语言模型（language model）
> 由P（wt|w1,···, wt-1）表示的模型称为条件语言模型（conditionallanguage model），有时也将其称为语言模型
* 语言模型（language model）给出了单词序列发生的概率。具体来说，就是使用概率来评估一个单词序列发生的可能性，即在多大程度上是自然的单词序列
* 使用语言模型，可以按照“作为句子是否自然”这一基准对候选句子进行排序
* 语言模型也可以用于生成新的句子

### 马尔可夫性
> 马尔可夫性是指未来的状态仅依存于当前状态。此外，当某个事件的概率仅取决于其前面的N个事件时，称为“N阶马尔可夫链”。

### word2vec与RNN
* CBOW模型还存在忽视了上下文中单词顺序的问题
* RNN具有一个机制，那就是无论上下文有多长，都能将上下文信息记住
* word2vec是以获取单词的分布式表示为目的的方法，因此一般不会用于语言模型

### 特征
* RNN的特征就在于拥有这样一个环路（或回路）。这个环路可以使数据不断循环。通过数据的循环，RNN一边记住过去的数据，一边更新到最新的数据
 <img src="https://user-images.githubusercontent.com/13389058/157156221-2ce10bbc-8349-45f0-959c-3b8e099fd05d.png" width="50%" height="50%">

### 公式
> 各个时刻的RNN层接收传给该层的输入和前一个RNN层的输出，然后据此计算当前时刻的输出，此时进行的计算可以用下式表示：**h<sub>t</sub>=tanh(h<sub>t-1</sub>W<sub>h</sub>+x<sub>t</sub>W<sub>x</sub>+b)**
* 时刻t的输入数据为x<sub>t</sub>
* RNN有两个权重，分别是将输入x转化为输出h的权重W<sub>x</sub>和将前一个RNN层的输出转化为当前时刻的输出的权重W<sub>h</sub>
* 偏置b
* ht-1和xt都是行向量
* tanh函数（双曲正切函数)
* 将RNN的输出h<sub>t</sub>称为隐藏状态（hidden state）或隐藏状态向量（hidden state vector）

### 基于时间的反向传播 BPTT
> 按时间顺序展开的神经网络的误差反向传播法,可以通过先进行正向传播，再进行反向传播的方式求目标梯度
* 随着时序数据的时间跨度的增大，BPTT消耗的计算机资源也会成比例地增大
* 反向传播的梯度也会变得不稳定
* 随着时序数据变长，计算机的内存使用量（不仅仅是计算量）也会增加

### 截断的BPTT (Truncated BPTT)
> 将时间轴方向上过长的网络在合适的位置进行截断，从而创建多个小型网络，然后对截出来的小型网络执行误差反向传播法. Truncated BPTT是指按适当长度截断的误差反向传播法
