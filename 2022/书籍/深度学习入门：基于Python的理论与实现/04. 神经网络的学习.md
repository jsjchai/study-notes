## 神经网络的学习
### 学习
> 这里所说的“学习”是指从训练数据中自动获取最优权重参数的过程

### 神经网络的特征
> 神经网络的特征就是可以从数据中学习。所谓“从数据中学习”，是指可以由数据自动决定权重参数的值

### 感知机收敛定理
> 通过有限次数的学习，线性可分问题是可解的。但是，非线性可分问题则无法通过（自动）学习来解决。

### 特征量
> 指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器

![image](https://user-images.githubusercontent.com/13389058/157394417-c9b6301f-5231-45c8-9217-5a3f32e4d70a.png)

### 训练数据
> 使用训练数据进行学习，寻找最优的参数

### 测试数据
> 使用测试数据评价训练得到的模型的实际能力

### 泛化能力
> 泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力

### 过拟合
> 只对某个数据集过度拟合的状态

### 损失函数
> 神经网络以某个指标为线索寻找最优权重参数。神经网络的学习中所用的指标称为损失函数（loss function）。这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等
* 损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致

#### one-hot表示
> 将正确解标签表示为1，其他标签表示为0的表示方法称为one-hot表示

#### 均方误差
![image](https://user-images.githubusercontent.com/13389058/157396403-158612f2-13f8-4551-a068-093dc06b86d7.png)


#### 交叉熵误差
![image](https://user-images.githubusercontent.com/13389058/157397973-fff18dcb-7dca-40bb-b625-0bb57675967a.png)
* 交叉熵误差的值是由正确解标签所对应的输出结果决定的

#### mini-batch学习
> 从全部数据中选出一部分，作为全部数据的“近似”。神经网络的学习也是从训练数据中选出一批数据（称为mini-batch,小批量），然后对每个mini-batch进行学习

#### 为何要设定损失函数
* 在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为0
* 损失函数不会为0,参数的值发生变化会发生连续性的变化

### 偏导数
> 有多个变量的函数的导数

### 梯度
> 由全部变量的偏导数汇总而成的向量称为梯度（gradient）
* 梯度会指向各点处的函数值降低的方向
* 梯度指示的方向是各点处的函数值减小最多的方向

#### 梯度法
> 通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法
* 机器学习的主要任务是在学习时寻找最优参数
* 神经网络在学习时找到最优参数（权重和偏置）
* 最优参数是指损失函数取最小值时的参数

#### 梯度下降法
> 寻找最小值的梯度法
* 一般来说，神经网络（深度学习）中，梯度法主要是指梯度下降法
#### 梯度上升法
> 寻找最大值的梯度法

#### 学习率
![image](https://user-images.githubusercontent.com/13389058/157818796-4a836d0c-10f9-4941-bcb1-6902da9a0ea3.png)
* 学习率过大的话，会发散成一个很大的值；反过来，学习率过小的话，基本上没怎么更新就结束
> 像学习率这样的参数称为超参数。这是一种和神经网络的参数（权重和偏置）性质不同的参数。相对于神经网络的权重参数是通过训练数据和学习算法自动获得的，学习率这样的超参数则是人工设定的。一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定
