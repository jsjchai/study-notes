## 神经网络
> 把最左边的一列称为输入层，最右边的一列称为输出层，中间的一列称为中间层。中间层有时也称为隐藏层。

![image](https://user-images.githubusercontent.com/13389058/157358685-15a24978-84c9-49db-b5c6-16959e134c4c.png)


### 激活函数
> 在计算网络中， 一个节点的激活函数定义了该节点在给定的输入或输入的集合下的输出。标准的计算机芯片电路可以看作是根据输入得到开（1）或关（0）输出的数字电路激活函数。这与神经网络中的线性感知机的行为类似。然而，只有非线性激活函数才允许这种网络仅使用少量节点来计算非平凡问题。 在人工神經网络中，这个功能也被称为传递函数

#### sigmoid函数
![image](https://user-images.githubusercontent.com/13389058/157359853-c78bad54-f154-4a1a-9020-617d764bd4fd.png)

#### 阶跃函数
> 式（3.3）表示的激活函数以阈值为界，一旦输入超过阈值，就切换输出。这样的函数称为“阶跃函数”。

![image](https://user-images.githubusercontent.com/13389058/157360055-5febf64d-bd3e-47c2-907d-330da5ae35a8.png)

#### 阶跃函数与sigmoid函数
* 阶跃函数和sigmoid函数还有其他共同点，就是两者均为非线性函数
* 为了发挥叠加层所带来的优势，激活函数必须使用非线性函数

#### ReLU（Rectified Linear Unit）函数
> ReLU函数在输入大于0时，直接输出该值；在输入小于等于0时，输出0

![image](https://user-images.githubusercontent.com/13389058/157382366-bc60c769-0fd7-4670-bd2c-5546e1c7e759.png)

### 输出层的设计
> 神经网络可以用在分类问题和回归问题上，不过需要根据情况改变输出层的激活函数。一般而言，回归问题用恒等函数，分类问题用softmax函数

#### 恒等函数
> 恒等函数会将输入按原样输出，对于输入的信息，不加以任何改动地直接输出

#### softmax函数
![image](https://user-images.githubusercontent.com/13389058/157385499-9e679590-80ca-4e73-b21a-f1b54c485387.png)
* softmax函数的输出是0.0到1.0之间的实数
* softmax函数的输出值的总和是1
* softmax函数的输出解释为“概率”
