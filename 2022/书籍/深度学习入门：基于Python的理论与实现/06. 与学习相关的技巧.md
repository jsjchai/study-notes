### 参数的更新
> 神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）
### 随机梯度下降法 SGD
> 使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法（stochastic gradient descent），简称SGD
![image](https://user-images.githubusercontent.com/13389058/158551173-d7cf782a-8b82-4df4-a24a-871cf27af027.png)

* SGD的缺点是，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效
* SGD低效的根本原因是，梯度的方向并没有指向最小值的方向

### Momentum
![image](https://user-images.githubusercontent.com/13389058/158552018-b8e22950-e27c-41fe-9ac3-6fdcf8941720.png)

### AdaGrad
#### 学习率衰减
> 随着学习的进行，使学习率逐渐减小
* AdaGrad进一步发展了这个想法，针对“一个一个”的参数，赋予其“定制”的值
![image](https://user-images.githubusercontent.com/13389058/158554009-001d5e1e-6e0f-4f97-8319-3e55bdfce21f.png)
* 可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小
* AdaGrad会记录过去所有梯度的平方和。因此，学习越深入，更新的幅度就越小。实际上，如果无止境地学习，更新量就会变为 0，完全不再更新
  *  解决方法 RMSProp方法 
      * RMSProp方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来。这种操作从专业上讲，称为“指数移动平均”，呈指数函数式地减小过去的梯度的尺度  

### Adam
> 融合了Momentum和AdaGrad的方法.Adam会设置 3个超参数。一个是学习率（论文中以α出现），另外两个是一次momentum系数β1和二次momentum系数β2。根据论文，标准的设定值是β1为 0.9，β2 为 0.999。设置了这些值后，大多数情况下都能顺利运行

### 权重的初始值
* 设定什么样的权重初始值，经常关系到神经网络的学习能否成功
* 权值衰减就是一种以减小权重参数的值为目的进行学习的方法。通过减小权重参数的值来抑制过拟合的发生
* 用作激活函数的函数最好具有关于原点对称的性质
* tanh函数和sigmoid函数同是S型曲线函数，但tanh函数是关于原点(0, 0)对称的S型曲线
* 当激活函数为sigmoid或tanh等S型曲线函数时，初始值使用Xavier初始值。这是目前的最佳实践。

### Batch Normalization
> Batch Norm的思路是调整各层的激活值分布使其拥有适当的广度

![image](https://user-images.githubusercontent.com/13389058/158559636-002af908-d3c2-4afd-bf11-5ba442a1368a.png)

* 可以使学习快速进行（可以增大学习率）
* 不那么依赖初始值（对于初始值不用那么神经质）
* 抑制过拟合（降低Dropout等的必要性）

### 正则化
#### 发生过拟合的原因
* 模型拥有大量参数、表现力强
* 训练数据少
#### 权值衰减
> 该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合
#### Dropout
> Dropout是一种在学习的过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递
* 正向传播时传递了信号的神经元，反向传播时按原样传递信号
* 正向传播时没有传递信号的神经元，反向传播时信号将停在那里

### 超参数的验证
> 这里所说的超参数是指，比如各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等
#### 验证数据
> 调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为验证数据（validation data）。我们使用这个验证数据来评估超参数的好坏
1. 设定超参数的范围
2. 从设定的超参数范围中随机采样
3. 使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别精度（但是要将epoch设置得很小）
4. 重复步骤1和步骤2（100次等），根据它们的识别精度的结果，缩小超参数的范围

#### 贝叶斯最优化（Bayesian optimization）
> 贝叶斯最优化运用以贝叶斯定理为中心的数学理论，能够更加严密、高效地进行最优化
