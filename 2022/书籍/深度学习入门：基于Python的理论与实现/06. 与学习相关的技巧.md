### 参数的更新
> 神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）
### 随机梯度下降法 SGD
> 使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法（stochastic gradient descent），简称SGD
![image](https://user-images.githubusercontent.com/13389058/158551173-d7cf782a-8b82-4df4-a24a-871cf27af027.png)

* SGD的缺点是，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效
* SGD低效的根本原因是，梯度的方向并没有指向最小值的方向

### Momentum
![image](https://user-images.githubusercontent.com/13389058/158552018-b8e22950-e27c-41fe-9ac3-6fdcf8941720.png)

### AdaGrad
#### 学习率衰减
> 随着学习的进行，使学习率逐渐减小
* AdaGrad进一步发展了这个想法，针对“一个一个”的参数，赋予其“定制”的值
![image](https://user-images.githubusercontent.com/13389058/158554009-001d5e1e-6e0f-4f97-8319-3e55bdfce21f.png)
* 可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小
* AdaGrad会记录过去所有梯度的平方和。因此，学习越深入，更新的幅度就越小。实际上，如果无止境地学习，更新量就会变为 0，完全不再更新
  *  解决方法 RMSProp方法 
      * RMSProp方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来。这种操作从专业上讲，称为“指数移动平均”，呈指数函数式地减小过去的梯度的尺度  

### Adam
> 融合了Momentum和AdaGrad的方法.Adam会设置 3个超参数。一个是学习率（论文中以α出现），另外两个是一次momentum系数β1和二次momentum系数β2。根据论文，标准的设定值是β1为 0.9，β2 为 0.999。设置了这些值后，大多数情况下都能顺利运行
