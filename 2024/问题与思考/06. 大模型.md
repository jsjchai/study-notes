## 底座
> 通常指的是大型语言模型（Large Language Model，LLM）的基础版本，它们是构建特定领域模型（如医疗大模型）的起点。
### 不同的底座模型差别
1. 参数量：不同底座模型的参数数量可能不同，如上文提到的2.7B和7B，这代表了模型的规模和复杂度。
2. 架构设计：底座模型可能采用不同的神经网络架构，如Transformer、BERT、GPT等，这些架构决定了模型处理数据的方式。
3. 预训练数据：底座模型在预训练阶段使用的数据集可能不同，这影响了模型学习到的知识和能力。
4. 微调方法：即使是相同规模的底座模型，采用的微调方法（如指令微调、持续预训练等）也可能导致性能和应用场景的差异。
5. 领域适应性：底座模型在经过特定领域的微调后，其对特定领域数据的理解和生成能力会有显著差异。
6. 性能：不同底座模型在特定任务上的表现可能不同，这取决于它们的设计、训练过程和微调策略。
7. 资源需求：不同规模和复杂度的底座模型对计算资源的需求也不同，大规模模型通常需要更多的计算能力和存储空间。
8. 开源与闭源：一些底座模型可能是开源的，允许研究者和开发者自由使用和修改；而另一些可能是闭源的，使用受限或需要特定的许可。
9. 社区支持和生态：不同底座模型背后的社区支持和生态系统的完善程度也会影响开发者的选择和模型的应用。
