## 无监督学习
### k均值聚类算法 (k-Means算法)
> k均值聚类算法（k-means clustering algorithm）是一种迭代求解的聚类分析算法，其步骤是，预将数据分为K组，则随机选取K个对象作为初始的聚类中心，然后计算每个对象与各个种子聚类中心之间的距离，把每个对象分配给距离它最近的聚类中心。聚类中心以及分配给它们的对象就代表一个聚类。每分配一个样本，聚类的聚类中心会根据聚类中现有的对象被重新计算。这个过程将不断重复直到满足某个终止条件。终止条件可以是没有（或最小数目）对象被重新分配给不同的聚类，没有（或最小数目）聚类中心再发生变化，误差平方和局部最小

![image](https://user-images.githubusercontent.com/13389058/154975956-407f9a2f-6816-471a-bc47-3690dc1c2266.png)

#### 算法目标

![image](https://user-images.githubusercontent.com/13389058/154979193-e4317b0a-d9d1-4bbf-92af-724aa1d047c9.png)


### 优化目标

![image](https://user-images.githubusercontent.com/13389058/154978009-0043c700-f0d8-4e5e-b42b-91a9b9b90879.png)


### 随机初始化
> K均值算法执行开始时，通常随机初始化聚类中心点，即：随机选择K个训练实例，然后令K个聚类中心分别等于这K个训练实例。这就使得Kmeans算法存在一个缺陷：最后结果会依赖于初始化的情况，并且有可能使得代价函数停留在局部最小值处

> 为了解决该问题，我们通常需要多次（50到1000次）运行K均值算法，每一次都重新进行初始化，最后再比较多次运行K-均值的结果，选择代价函数最小的结果。通常这种方法在K较小的时候（2-10）还是可行的；但是K较大，这么做可能不会有明显的改善，并且K较大时，通常第一次执行K均值也会得到一个不错的结果。典型的执行次数为100次

### 合理选择K值
* K值的选取对 K-means 影响很大，这也是 K-means 最大的缺点，常见的选取 K 值的方法有：手肘法、Gap statistic方法

#### 肘部法则（Elbow method）
> 该方法所做的就是不断的改变K值（from 1 to x），执行k-均值，然后画出代价函数与K值的变化曲线，选择“肘点处”的值作为K的取值

![image](https://user-images.githubusercontent.com/13389058/154977669-84c809b9-5916-4497-8089-7c7a8c01d8a6.png)

### 数据降维
> 在机器学习和统计学领域，降维是指在某些限定条件下，降低随机变量个数，得到一组“不相关”主变量的过程
#### 变量选择
> 变量选择假定数据中包含大量冗余或无关变量（或称特征、属性、指标等），旨在从原有变量中找出主要变量
#### 特征提取
> 特征提取可以看作变量选择方法的一般化：变量选择假设在原始数据中，变量数目浩繁，但只有少数几个真正起作用；而特征提取则认为在所有变量可能的函数(比如这些变量各种可能的线性组合)中，只有少数几个真正起作用
