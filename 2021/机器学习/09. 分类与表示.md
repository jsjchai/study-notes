
## 分类问题
> 分类相信大家都不会陌生，生活中会见到很多的应用，比如垃圾邮件识别、信用卡发放等等，就是基于数据集，作出二分类或者多分类的选择

## 逻辑函数(Logistic Function)/S型函数(Sigmoid Function)
### 公式
> h<sub>θ</sub>(x)=g(θ<sup>T</sup>x)

> z=θ<sup>T</sup>x

> g(z)=1/1+e<sup>-z</sup>

### 函数曲线
![image](https://user-images.githubusercontent.com/13389058/145672042-126811d4-eed4-498e-9508-68402067a423.png)

### 概率
* h<sub>θ</sub> = P(y=1|x;θ)= 1 - P(y=0|x;θ)
* P(y=1|x;θ) + P(y=0|x;θ) = 1

### 决策边界
> 决策边界或决策面（decision surface）是统计分类问题中的一个超曲面，把向量空间（作为特征空间）划分为两个集合，分别对应两个分类。如果决策面是超平面，那么这个分类问题是线性的，分类是线性可分的

### 成本函数
![image](https://user-images.githubusercontent.com/13389058/145698823-069a9f20-a89a-4dfd-8991-834b0407f8d9.png)
![image](https://user-images.githubusercontent.com/13389058/145698837-734ed24c-4f02-4e87-ad08-bc2406b0845d.png)
![image](https://user-images.githubusercontent.com/13389058/145698845-7413dec4-7fb4-43ed-8d3c-8336178d70bf.png)
![image](https://user-images.githubusercontent.com/13389058/145698850-192a80e4-005b-467e-9042-e0a88fe31b8f.png)

### 简化的成本函数和梯度下降
* Cost(h<sub>θ</sub>(x),y)= -ylog(h<sub>θ</sub>(x))-(1-y)log(1-h<sub>θ</sub>(x))

![image](https://user-images.githubusercontent.com/13389058/145699251-15d11aa5-225c-431d-9d3d-45fd0743817e.png)
![image](https://user-images.githubusercontent.com/13389058/145699279-73bef252-17cb-49ba-958d-d0413320bcf7.png)

### 高级优化
#### 更复杂、更快速的优化θ的方法，可以用来代替梯度下降
* Conjugate gradient
* BFGS
* L-BFGS
